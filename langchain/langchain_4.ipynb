{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading OpenAI Key\n",
    "OPENAI_API_KEY=input(\"Enter OpenAI API Key\")\n",
    "#Reading Langchain API Key\n",
    "LANGCHAIN_API_KEY=input(\"Enter Langchain API Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enabiling Langchain Tracing\n",
    "LANGCHAIN_TRACING_V2= \"true\"\n",
    "#OpenAI Model\n",
    "OPENAI_MODEL=\"gpt-4o-mini\"\n",
    "LANGCHAIN_TRACING_V2=\"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing langchain_openai package\n",
    "! pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10ed073b0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10ed07a40>, root_client=<openai.OpenAI object at 0x10ed38470>, root_async_client=<openai.AsyncOpenAI object at 0x10ed07380>, model_name='gpt-4o-mini', temperature=0.5, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing llm\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=OPENAI_MODEL,openai_api_key=OPENAI_API_KEY,temperature=0.5)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Python package for SQLDatabase\n",
    "! pip install langchain-community==0.2.9\n",
    "! pip install sqlalchemy==2.0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.utilities.sql_database.SQLDatabase at 0x1184d3920>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initializing Database object\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "db = SQLDatabase.from_uri(\"sqlite:///baggage_tracking.db\")\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import BaseGenerationOutputParser\n",
    "from typing import List\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.outputs import  Generation\n",
    "class SQlExtractParser(BaseGenerationOutputParser[str]):\n",
    "    \"\"\"\n",
    "    A parser for SQL output.\n",
    "    \"\"\"\n",
    "\n",
    "    def parse_result(self, result: List[Generation], *, partial: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Parse the SQL query input and return the extracted SQL query.\n",
    "\n",
    "        Args:\n",
    "            sql_query_input (str): The input SQL query string.\n",
    "            partial (bool, optional): Whether the input is a partial query. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted SQL query.\n",
    "        \"\"\"\n",
    "        if not result:\n",
    "            raise OutputParserException(\"Input SQL query is empty\")\n",
    "        \n",
    "        print(f\"Incoming query {result[0]}\")\n",
    "\n",
    "        if 'SQLQuery:' in result[0].text:\n",
    "            sql_query = result[0].text.split('SQLQuery:', 1)[1].strip()\n",
    "            print(f\"Extracted SQL query: {sql_query}\")\n",
    "        else:\n",
    "            sql_query = result[0].text\n",
    "\n",
    "        return sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_examples = [\n",
    "    {\n",
    "        \"input\": \"Show bag tag numbers for the PNR number AAA100\",\n",
    "        \"query\": \"select bag_tag_number from baggage_details where pnr = 'AAA100';\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Find the baggage details of the flight originating from JFK and destination is SFO\",\n",
    "        \"query\": \"select * from baggage_details where origin ='JFK' and destination ='SFO';\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Find scanning details of bag tag number 001234567890\",\n",
    "        \"query\": \"SELECT * from baggage_scanning where bag_tag_number = '001234567890';\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Find the passengers in flight BC101\",\n",
    "        \"query\": \"SELECT * from passenger_details where passenger_flight_number = 'BC101';\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Find the passenger details for the bag tag number 001234567890\",\n",
    "        \"query\": \"SELECT * from passenger_details where bag_tag_number = '001234567890'\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Find rush bags in flight AB100\",\n",
    "        \"query\": \"SELECT * from baggage_details where rush_bag='Yes' and flight_number = 'AB100'\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain Example  Selector\n",
    "=====================================\n",
    "langchain_core.example_selectors is a module in LangChain that deals with selecting examples dynamically from a set of predefined examples based on the current input or prompt. This is useful when you want to choose the most relevant examples to show or use for few-shot learning, helping to improve the performance of large language models (LLMs) by providing them with more specific and contextual examples.\n",
    "\n",
    "Example Selectors dynamically pick the most relevant examples from a set, based on the input to the language model. Instead of hardcoding examples for few-shot learning, example selectors allow you to choose relevant examples based on input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# initializes an instance of a Chroma vector store.\n",
    "vector_store = Chroma()\n",
    "# deletes any existing collection in the vector store\n",
    "vector_store.delete_collection()\n",
    "#  LangChain class that helps you select examples based on their semantic similarity to a given input. \n",
    "# It uses vector embeddings (vectorized representations of text) to compare the input with examples.\n",
    "# creating creates an instance of SemanticSimilarityExampleSelector\n",
    "sql_example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    sql_examples,\n",
    "    # To transform SQL examples and input queries into embeddings (vectors), enabling semantic comparison between them.\n",
    "    OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY),\n",
    "    # This is the Chroma vector store, where embeddings of the examples will be stored and queried. \n",
    "    vector_store,\n",
    "    # This specifies that the selector should return the top 2 most similar examples. \n",
    "    k=2,\n",
    "    #  This indicates that the key \"input\" in the provided examples will be used as the query. \n",
    "    # It tells the selector what part of each example should be compared to the input query.\n",
    "    input_keys=[\"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "sql_example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\\nSQLQuery:\"),\n",
    "        (\"ai\",\"{query}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "sql_few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    # This is the format/template used for each individual example in the few-shot learning setup. \n",
    "    # It defines how each example (SQL query and its corresponding result) will be presented.\n",
    "    example_prompt=sql_example_prompt,\n",
    "    # example selector that dynamically chooses the most relevant examples based on the user's input.\n",
    "    example_selector=sql_example_selector,\n",
    "    # This indicates which variables (placeholders) from the input will be used in the prompt template.\n",
    "    input_variables=[\"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_QUERY_PROMPT=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"\"\" You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\n",
    "        Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\n",
    "        Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\n",
    "        Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "        Pay attention to use date('now') function to get the current date, if the question involves \"today\".\n",
    "\n",
    "        Use the following format:\n",
    "\n",
    "        Question: Question here\n",
    "        SQLQuery: SQL Query to run\n",
    "        SQLResult: Result of the SQLQuery\n",
    "         \n",
    "        {top_k}\n",
    "\n",
    "        Only use the following tables:\n",
    "        {table_info}\"\"\"),\n",
    "        sql_few_shot_prompt,\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "ANSWER_PROMPTS = PromptTemplate.from_template(\n",
    "     \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
    "     Combine the the Question and SQL Result to generate a meaningful answer to the user.\n",
    "     The answer should be in the format of a sentence.\n",
    "     The answer should be a direct response to the user question.\n",
    "     Respond to the question as an experienced airline ground staff\n",
    "\n",
    " Question: {question}\n",
    " SQL Query: {query}\n",
    " SQL Result: {result}\n",
    " Answer: \"\"\"\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Table Selection\n",
    "========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "TABLE_SELECTION_PROMPT=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"\"\"Return the names of ALL the SQL tables that MIGHT be relevant to the user question.\n",
    "        The tables are:\n",
    "        {table_names}\n",
    "        Remember to include ALL POTENTIALLY RELEVANT tables, even if you're not sure that they're needed.\n",
    "        Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n",
    "        \"\"\"),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import BaseGenerationOutputParser\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.outputs import Generation\n",
    "from typing import List\n",
    "class TableNameExtractParser(BaseGenerationOutputParser[str]):\n",
    "\n",
    "    def parse_result(self, result: List[Generation], *, partial: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Parse the result and return a comma-separated list of table names.\n",
    "\n",
    "        Args:\n",
    "            result (List[Generation]): The input list of generations.\n",
    "            partial (bool, optional): Whether to parse partially. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            str: A comma-separated list of table names.\n",
    "\n",
    "        Raises:\n",
    "            OutputParserException: If the input table list is empty.\n",
    "        \"\"\"\n",
    "        if not result:\n",
    "            raise OutputParserException(\"Input Table list is empty\")\n",
    "        \n",
    "        print(\"Table list XA %s\",result[0])\n",
    "        output_parser = CommaSeparatedListOutputParser()\n",
    "        table_list = output_parser.parse_result(result) \n",
    "\n",
    "        return table_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "table_selection_chain={\"input\":itemgetter(\"question\"),\"table_names\":itemgetter(\"table_names\")}|TABLE_SELECTION_PROMPT|llm|TableNameExtractParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "table_details = pd.read_csv(\"./table_details.csv\")\n",
    "\n",
    "table_description_array = []\n",
    "for index, row in table_details.iterrows():\n",
    "    table_description_array.append(\"Table Name:\" + row['TableName'] + \"\\n\" + \"Table Description:\" + row['Description'] + \"\\n\\n\")\n",
    "\n",
    "table_description = '\\n'.join(table_description_array)\n",
    "print(\"Table description \", table_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table list XA %s text='passenger_details' message=AIMessage(content='passenger_details', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 256, 'total_tokens': 259}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-dba2463f-e295-4e63-86da-968ca7469b24-0', usage_metadata={'input_tokens': 256, 'output_tokens': 3, 'total_tokens': 259})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['passenger_details']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_selection_chain.invoke({\"question\":\"On an average how many passengers are there in a flight?\",\n",
    "                             \"table_names\":table_description})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=RunnableAssign(mapper={\n",
      "  table_names_to_use: {\n",
      "                        input: RunnableLambda(itemgetter('question')),\n",
      "                        table_names: RunnableLambda(itemgetter('table_names'))\n",
      "                      }\n",
      "                      | ChatPromptTemplate(input_variables=['input', 'table_names'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['table_names'], template=\"Return the names of ALL the SQL tables that MIGHT be relevant to the user question.\\n        The tables are:\\n        {table_names}\\n        Remember to include ALL POTENTIALLY RELEVANT tables, even if you're not sure that they're needed.\\n        Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\\n        \")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
      "                      | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10ed073b0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10ed07a40>, root_client=<openai.OpenAI object at 0x10ed38470>, root_async_client=<openai.AsyncOpenAI object at 0x10ed07380>, model_name='gpt-4o-mini', temperature=0.5, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
      "                      | TableNameExtractParser()\n",
      "}) middle=[RunnableAssign(mapper={\n",
      "  query: RunnableAssign(mapper={\n",
      "           input: RunnableLambda(...),\n",
      "           table_info: RunnableLambda(...)\n",
      "         })\n",
      "         | RunnableLambda(lambda x: {k: v for k, v in x.items() if k not in ('question', 'table_names_to_use')})\n",
      "         | ChatPromptTemplate(input_variables=['input', 'table_info'], partial_variables={'top_k': '5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['table_info', 'top_k'], template=' You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\\n        Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\\n        Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\n        Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\n        Pay attention to use date(\\'now\\') function to get the current date, if the question involves \"today\".\\n\\n        Use the following format:\\n\\n        Question: Question here\\n        SQLQuery: SQL Query to run\\n        SQLResult: Result of the SQLQuery\\n         \\n        {top_k}\\n\\n        Only use the following tables:\\n        {table_info}')), FewShotChatMessagePromptTemplate(example_selector=SemanticSimilarityExampleSelector(vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x11f4afcb0>, k=2, example_keys=None, input_keys=['input'], vectorstore_kwargs=None), input_variables=['input'], example_prompt=ChatPromptTemplate(input_variables=['input', 'query'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}\\nSQLQuery:')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], template='{query}'))])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
      "         | RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10ed073b0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10ed07a40>, root_client=<openai.OpenAI object at 0x10ed38470>, root_async_client=<openai.AsyncOpenAI object at 0x10ed07380>, model_name='gpt-4o-mini', temperature=0.5, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'stop': ['\\nSQLResult:']})\n",
      "         | StrOutputParser()\n",
      "         | RunnableLambda(_strip)\n",
      "         | SQlExtractParser()\n",
      "}), RunnableAssign(mapper={\n",
      "  result: RunnableLambda(itemgetter('query'))\n",
      "          | QuerySQLDataBaseTool(db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x1184d3920>)\n",
      "}), PromptTemplate(input_variables=['query', 'question', 'result'], template='Given the following user question, corresponding SQL query, and SQL result, answer the user question.\\n     Combine the the Question and SQL Result to generate a meaningful answer to the user.\\n     The answer should be in the format of a sentence.\\n     The answer should be a direct response to the user question.\\n     Respond to the question as an experienced airline ground staff\\n\\n Question: {question}\\n SQL Query: {query}\\n SQL Result: {result}\\n Answer: '), ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10ed073b0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10ed07a40>, root_client=<openai.OpenAI object at 0x10ed38470>, root_async_client=<openai.AsyncOpenAI object at 0x10ed07380>, model_name='gpt-4o-mini', temperature=0.5, openai_api_key=SecretStr('**********'), openai_proxy='')] last=StrOutputParser()\n",
      "Table list XA %s text='passenger_details' message=AIMessage(content='passenger_details', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 256, 'total_tokens': 259}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-a45591ba-196b-4ec1-acd0-18f00bd798cb-0', usage_metadata={'input_tokens': 256, 'output_tokens': 3, 'total_tokens': 259})\n",
      "Incoming query text='SELECT \"passenger_flight_number\", COUNT(\"passenger_id\") AS \"passenger_count\" \\nFROM passenger_details \\nGROUP BY \"passenger_flight_number\";'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "# Create a query chain\n",
    "query_chain = create_sql_query_chain(llm, db,prompt=SQL_QUERY_PROMPT) | SQlExtractParser()\n",
    "\n",
    "# Create an execution chain\n",
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "execute_chain = itemgetter(\"query\") | execute_query\n",
    "\n",
    "# Create an answer chain\n",
    "answer_chain = ANSWER_PROMPTS | llm | StrOutputParser()\n",
    "\n",
    "# Combine the chains\n",
    "final_chain = (\n",
    "    RunnablePassthrough()\n",
    "    .assign(table_names_to_use=table_selection_chain)\n",
    "    .assign(query=query_chain)\n",
    "    .assign(result=execute_chain)\n",
    "    | answer_chain\n",
    ")\n",
    "\n",
    "# Invoke the chain with the user's query\n",
    "input_data = {\n",
    "    \"question\": \"On an average how many passengers are there in a flight?\",\n",
    "    \"dialect\": \"sqlite\",\n",
    "    \"top_k\":2,\n",
    "    \"table_names\": table_description,\n",
    "}\n",
    "#answer=query_chain.invoke(input_data)\n",
    "print(final_chain)\n",
    "answer = final_chain.invoke(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On average, there are about 1 to 2 passengers per flight based on the recent data from our flight records.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
